{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'public_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m         public_tweets \u001b[38;5;241m=\u001b[39m status\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m \n\u001b[1;32m---> 23\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[43mpublic_tweets\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#convert to string\u001b[39;00m\n\u001b[0;32m     26\u001b[0m json_str \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(status\u001b[38;5;241m.\u001b[39m_json)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'public_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "consumer_key = 'w18YGO9bsufdhkSkL5gCugAfA'\n",
    "consumer_secret = 'xQGMpJvwjKLLMec8AfXK94Oo3d7dC5SXNCQJ8suelfwFjk8hBl'\n",
    "access_key= '1595635150804525056-4uIg75VX9E3N4zC0XJRBBnjd1CanTv'\n",
    "access_secret = 'lKViDwCfrHV5WrLf7v8XAQLhz5AYXoaZSrPc3wvVYrZaL'\n",
    "\n",
    "start_date = '2022-11-20'\n",
    "end_date = '2022-11-21'\n",
    "\n",
    "#pass twitter credentials to tweepy\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "a = tweepy.Cursor(api.search_tweets, q='piala_dunia',\n",
    "              count=1, tweet_mode=\"extended\").pages(1)\n",
    "for page in a:\n",
    "    for status in page:\n",
    "        public_tweets = status\n",
    "        \n",
    "import json \n",
    "\n",
    "status = public_tweets\n",
    "\n",
    "#convert to string\n",
    "json_str = json.dumps(status._json)\n",
    "\n",
    "#deserialise string into python object\n",
    "parsed = json.loads(json_str)\n",
    "\n",
    "b = json.dumps(parsed, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "file_2  = open(\"cleaning_source/combined_slang_words.txt\", \"r\")\n",
    "content2 = file_2.read()\n",
    "slang_words = ast.literal_eval(content2)\n",
    "file_2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slang_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_words['gak']  = 'tidak'\n",
    "slang_words['trus'] = 'lalu'\n",
    "slang_words['dlm'] = 'dalam'\n",
    "slang_words['tetep'] = 'tetap'\n",
    "slang_words['skrg'] = 'sekarang'\n",
    "slang_words['sm'] = 'dengan'\n",
    "slang_words['udh'] = 'sudah'\n",
    "slang_words['cm'] = 'cuma'\n",
    "slang_words['org'] = 'orang'\n",
    "slang_words['bangor'] = 'nakal'\n",
    "slang_words['ngamuk'] = 'mengamuk'\n",
    "slang_words['iso'] = 'bisa'\n",
    "slang_words['mbuh'] = 'tidak tahu'\n",
    "slang_words['kzl'] = 'kesal'\n",
    "slang_words['name'] = 'nama'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"cleaning_source/update_combined_slang_words.txt\",\"w\")\n",
    "f.write( str(slang_words) )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "file_2  = open(\"cleaning_source/update_combined_slang_words.txt\", \"r\")\n",
    "content2 = file_2.read()\n",
    "update_slang_words = ast.literal_eval(content2)\n",
    "file_2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(update_slang_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "import pytz \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lexicon = pd.read_csv('lexicon/InSet-master/positive.tsv',sep='\\t')\n",
    "neg_lexicon = pd.read_csv('lexicon/InSet-master/negative.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = pos_lexicon.append(neg_lexicon,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addition = pd.read_csv('lexicon/agusmakmun/sentimentword.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addition.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_word = lexicon['word'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon['word'][0] in lexicon_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_word = []\n",
    "add_weight = []\n",
    "for i in range(0,len(addition)):\n",
    "    if (addition['word'][i] not in lexicon_word):\n",
    "        add_word.append(addition['word'][i])\n",
    "        add_weight.append(addition['weight'][i])\n",
    "\n",
    "addition_lexicon = pd.DataFrame(list(zip(add_word,add_weight)),columns =['word','weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addition_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon = lexicon.append(addition_lexicon,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon[full_lexicon['weight']<0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = open(\"lexicon/swearwords/swear-words.txt\", \"r\")\n",
    "content = my_file.read()\n",
    "swear_words = content.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_swear = [-5 for i in range(len(swear_words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swear_lexicon = pd.DataFrame(list(zip(swear_words,weight_swear)),columns =['word','weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swear_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon = lexicon.append(swear_lexicon,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_words(x):\n",
    "    words = word_tokenize(x['word'])\n",
    "    number = len(words)\n",
    "    return number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon['number_of_words'] = full_lexicon.apply(lambda x: number_of_words(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon = full_lexicon.drop(full_lexicon[full_lexicon['number_of_words'] == 0].index[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon  = full_lexicon.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon[full_lexicon['number_of_words'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon.to_csv(r'lexicon\\full_lexicon.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_lexicon = pd.read_csv('lexicon/modified_full_lexicon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_lexicon['number_of_words'] = modified_lexicon.apply(lambda x: number_of_words(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_lexicon.to_csv(r'lexicon\\modified_full_lexicon.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = open(\"cleaning_source/combined_stop_words.txt\", \"r\")\n",
    "content = my_file.read()\n",
    "stop_words = content.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_word = modified_lexicon['word'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap=[]\n",
    "for word in stop_words:\n",
    "    if word in lexicon_word:\n",
    "        overlap.append(word)\n",
    "    else:\n",
    "        kata_dasar = stemmer.stem(word)\n",
    "        if kata_dasar in lexicon_word:\n",
    "            overlap.append(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c5f9eb9d72c0934fd2896ee225763aa5edf440926a0c875311ee0bd5be674b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
