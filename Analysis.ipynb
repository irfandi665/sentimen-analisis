{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "import pytz \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import ast\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_extraction/pildun3.csv')\n",
    "\n",
    "jakarta = pytz.timezone('Asia/Jakarta')\n",
    "df['datetime_created'] = df['created_at'].apply(lambda x: datetime.strptime(x,'%a %b %d %H:%M:%S %z %Y').replace(tzinfo=pytz.UTC).astimezone(jakarta))\n",
    "df['date_created'] = df['datetime_created'].apply(lambda x: x.date())\n",
    "df['time_created'] = df['datetime_created'].apply(lambda x: x.time())\n",
    "df = df.drop(['datetime_created'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Duplicate if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['clean_text'].isnull()==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df[df['clean_text'].isnull()==True]['original_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menghapus pengamatan bahwa teks menjadi kosong setelah dibersihkan, menunjukkan bahwa teks tidak memiliki sentimen penting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['clean_text'])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['lang']!='in']['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena ada beberapa kata yang tidak dalam bahasa Indonesia, mungkin orang Indonesia menggunakan bahasa lain untuk tweet, teks tersebut akan diterjemahkan untuk memproses teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(x,src):\n",
    "    translator = Translator()\n",
    "    try:\n",
    "        sentence = translator.translate(x, src=src,dest='id').text\n",
    "    except:\n",
    "        sentence = x\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df.apply(lambda x: trans(x['clean_text'],x['lang']) if(x['lang']!='in') else x['clean_text'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = df['clean_text'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "clean_text.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_exaggeration(x):\n",
    "    word_tokens = word_tokenize(x)\n",
    "    new_x =''\n",
    "    for i in word_tokens:\n",
    "        if (i =='psbb'):\n",
    "            new = re.sub(r'(\\w)\\1\\1+',r'\\1\\1',i)\n",
    "            new_x = new_x +new+' '\n",
    "        elif(i =='psb'):\n",
    "            new = 'psbb'\n",
    "            new_x = new_x +new+' '\n",
    "        else:\n",
    "            new = re.sub(r'(\\w)\\1\\1\\1+',r'\\1',i)\n",
    "            new_x = new_x +new+' '\n",
    "    return new_x\n",
    "\n",
    "def del_word(x,key_list):\n",
    "    n = len(key_list)\n",
    "    word_tokens = word_tokenize(x)\n",
    "    new_x =''\n",
    "    for word in word_tokens:\n",
    "        if word not in key_list:\n",
    "            new_x = new_x+word+' '\n",
    "    return new_x\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "   # nltk.download('stopwords')\n",
    "    my_file = open(\"cleaning_source/combined_stop_words.txt\", \"r\")\n",
    "    content = my_file.read()\n",
    "    stop_words = content.split(\"\\n\")\n",
    "    file_2  = open(\"cleaning_source/update_combined_slang_words.txt\", \"r\")\n",
    "    content2 = file_2.read()\n",
    "    slang_words = ast.literal_eval(content2)\n",
    "    my_file.close()\n",
    "    file_2.close()\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "    #after tweepy preprocessing the colon left remain after removing mentions\n",
    "    #or RT sign in the beginning of the tweet\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "\n",
    "    #remove emojis from tweet\n",
    "    #tweet = emoji_pattern.sub(r'', tweet)\n",
    "    \n",
    "    #remove punctuation manually\n",
    "    tweet = re.sub('[^a-zA-Z]', ' ', tweet)\n",
    "    \n",
    "    #remove tags\n",
    "    tweet=re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",tweet)\n",
    "    \n",
    "    #remove digits and special chars\n",
    "    tweet=re.sub(\"(\\\\d|\\\\W)+\",\" \",tweet)\n",
    "\n",
    "    #remove other symbol from tweet\n",
    "    tweet = re.sub(r'â', '', tweet)\n",
    "    tweet = re.sub(r'€', '', tweet)\n",
    "    tweet = re.sub(r'¦', '', tweet)\n",
    "\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    for w in word_tokens:\n",
    "        if w in slang_words.keys():\n",
    "            word_tokens[word_tokens.index(w)] = slang_words[w]\n",
    "\n",
    "    #filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    "\n",
    "    #looping through conditions\n",
    "    for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in string.punctuation:\n",
    "            filtered_tweet.append(w.lower())\n",
    "    return ' '.join(filtered_tweet)\n",
    "\n",
    "def count_words(x):\n",
    "    words = word_tokenize(x)\n",
    "    n=len(words)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text that exaggerate the typing such as 'psbbbbbbbbb' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_exag = clean_text.apply(lambda x: repair_exaggeration(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_exag.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recleaning after remove exaggeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_clean = clean_text_exag.apply(lambda x: clean_tweets(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kami menggunakan kata kunci untuk meminta data, sekarang kami perlu menghapusnya karena semuanya akan ditemukan di setiap kalimat dalam bingkai data ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = ['psbb','psb','corona','covid19','indonesia','pemerintah','wfh','covid']\n",
    "clean_text_extra = re_clean.apply(lambda x: del_word(x,keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_extra.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = clean_text_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_length'] = df['clean_text'].apply(lambda x:count_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_length'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df['word_length']==0].index,axis=0)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for i in range(0,len(df['clean_text'])):\n",
    "    sentence = df['clean_text'][i]\n",
    "    word_token = word_tokenize(sentence)\n",
    "    for j in word_token:\n",
    "        if j not in word_dict:\n",
    "            word_dict[j] = 1\n",
    "        else:\n",
    "            word_dict[j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len({k:v for (k,v) in word_dict.items() if v < 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Lexicon data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impor leksikon, dan hapus kata -kata negasi dari leksikon, leksikon adalah kombinasi dari beberapa sumber di bawah ini, yang digabungkan bersama, dan termasuk kata -kata bersumpah yang memiliki skor paling negatif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sources : <br>\n",
    "https://github.com/louisowen6/NLP_bahasa_resources <br>\n",
    "https://github.com/abhimantramb/elang/blob/master/word2vec/utils/swear-words.txt <br>\n",
    "https://github.com/fajri91/InSet <br>\n",
    "https://github.com/agusmakmun/SentiStrengthID/blob/master/id_dict/sentimentword.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negasi = ['bukan','tidak','ga','gk']\n",
    "lexicon = pd.read_csv('lexicon/modified_full_lexicon.csv')\n",
    "lexicon = lexicon.drop(lexicon[(lexicon['word'] == 'bukan')\n",
    "                               |(lexicon['word'] == 'tidak')\n",
    "                               |(lexicon['word'] == 'ga')|(lexicon['word'] == 'gk') ].index,axis=0)\n",
    "lexicon = lexicon.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_word = lexicon['word'].to_list()\n",
    "lexicon_num_words = lexicon['number_of_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lexicon_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memeriksa apakah ada kata -kata dalam kamus yang tidak termasuk dalam leksikon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_words = []\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "for word in word_dict.keys():\n",
    "    if word not in lexicon_word:\n",
    "        kata_dasar = stemmer.stem(word)\n",
    "        if kata_dasar not in lexicon_word:\n",
    "            ns_words.append(word)\n",
    "len(ns_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita lihat kata -kata seperti apa mereka, mari kita mulai dengan beberapa kata yang memiliki banyak kejadian karena ini kemungkinan besar bukan tipe case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len({k:v for (k,v) in word_dict.items() if ((k in ns_words)&(v>3)) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_words_list = {k:v for (k,v) in word_dict.items() if ((k in ns_words)&(v>3))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ternyata kata -kata yang tidak termasuk dalam leksikon, adalah orang yang tidak memiliki sentimen signifikan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_orders = sorted(ns_words_list.items(), key=lambda x: x[1], reverse=True)\n",
    "sort_orders=sort_orders[0:20]\n",
    "for i in sort_orders:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_plot = df['clean_text'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_plot_1 = word_to_plot.apply(lambda x: del_word(x,negasi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat kata cloud untuk melihat kata -kata seperti apa yang sering muncul di tweet yang terkait dengan pandemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 800, height = 800, background_color = 'black', max_words = 1000\n",
    "                      , min_font_size = 20).generate(str(word_to_plot_1))\n",
    "#plot the word cloud\n",
    "fig = plt.figure(figsize = (8,8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon['number_of_words'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'pekerti' in word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'budi baik' in lexicon_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menghitung sentimen kata -kata dengan memasukkannya ke leksikon sambil juga membuat kantong kata -kata matriks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sencol =[]\n",
    "senrow =np.array([])\n",
    "nsen = 0\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "sentiment_list = []\n",
    "# function to write the word's sentiment if it is founded\n",
    "def found_word(ind,words,word,sen,sencol,sentiment,add):\n",
    "    # if it is already included in the bag of words matrix, then just increase the value\n",
    "    if word in sencol:\n",
    "        sen[sencol.index(word)] += 1\n",
    "    else:\n",
    "    #if not, than add new word\n",
    "        sencol.append(word)\n",
    "        sen.append(1)\n",
    "        add += 1\n",
    "    #if there is a negation word before it, the sentiment would be the negation of it's sentiment\n",
    "    if (words[ind-1] in negasi):\n",
    "        sentiment += -lexicon['weight'][lexicon_word.index(word)]\n",
    "    else:\n",
    "        sentiment += lexicon['weight'][lexicon_word.index(word)]\n",
    "    \n",
    "    return sen,sencol,sentiment,add\n",
    "            \n",
    "# checking every words, if they are appear in the lexicon, and then calculate their sentiment if they do\n",
    "for i in range(len(df)):\n",
    "    nsen = senrow.shape[0]\n",
    "    words = word_tokenize(df['clean_text'][i])\n",
    "    sentiment = 0 \n",
    "    add = 0\n",
    "    prev = [0 for ii in range(len(words))]\n",
    "    n_words = len(words)\n",
    "    if len(sencol)>0:\n",
    "        sen =[0 for j in range(len(sencol))]\n",
    "    else:\n",
    "        sen =[]\n",
    "    \n",
    "    for word in words:\n",
    "        ind = words.index(word)\n",
    "        # check whether they are included in the lexicon\n",
    "        if word in lexicon_word :\n",
    "            sen,sencol,sentiment,add= found_word(ind,words,word,sen,sencol,sentiment,add)\n",
    "        else:\n",
    "        # if not, then check the root word\n",
    "            kata_dasar = stemmer.stem(word)\n",
    "            if kata_dasar in lexicon_word:\n",
    "                sen,sencol,sentiment,add= found_word(ind,words,kata_dasar,sen,sencol,sentiment,add)\n",
    "        # if still negative, try to match the combination of words with the adjacent words\n",
    "            elif(n_words>1):\n",
    "                if ind-1>-1:\n",
    "                    back_1    = words[ind-1]+' '+word\n",
    "                    if (back_1 in lexicon_word):\n",
    "                        sen,sencol,sentiment,add= found_word(ind,words,back_1,sen,sencol,sentiment,add)\n",
    "                    elif(ind-2>-1):\n",
    "                        back_2    = words[ind-2]+' '+back_1\n",
    "                        if back_2 in lexicon_word:\n",
    "                            sen,sencol,sentiment,add= found_word(ind,words,back_2,sen,sencol,sentiment,add)\n",
    "    # if there is new word founded, then expand the matrix\n",
    "    if add>0:  \n",
    "        if i>0:\n",
    "            if (nsen==0):\n",
    "                senrow = np.zeros([i,add],dtype=int)\n",
    "            elif(i!=nsen):\n",
    "                padding_h = np.zeros([nsen,add],dtype=int)\n",
    "                senrow = np.hstack((senrow,padding_h))\n",
    "                padding_v = np.zeros([(i-nsen),senrow.shape[1]],dtype=int)\n",
    "                senrow = np.vstack((senrow,padding_v))\n",
    "            else:\n",
    "                padding =np.zeros([nsen,add],dtype=int)\n",
    "                senrow = np.hstack((senrow,padding))\n",
    "            senrow = np.vstack((senrow,sen))\n",
    "        if i==0:\n",
    "            senrow = np.array(sen).reshape(1,len(sen))\n",
    "    # if there isn't then just update the old matrix\n",
    "    elif(nsen>0):\n",
    "        senrow = np.vstack((senrow,sen))\n",
    "        \n",
    "    sentiment_list.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentiment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(senrow.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membangun bingkai data yang berisi kantong kata dan sentimen yang telah dihitung sebelumnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sencol.append('sentiment')\n",
    "sentiment_array = np.array(sentiment_list).reshape(senrow.shape[0],1)\n",
    "sentiment_data = np.hstack((senrow,sentiment_array))\n",
    "df_sen = pd.DataFrame(sentiment_data,columns = sencol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sen.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita lihat apakah sentimennya benar dengan melihat teks aslinya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cek_df = pd.DataFrame([])\n",
    "cek_df['text'] = df['original_text'].copy()\n",
    "cek_df['sentiment']  = df_sen['sentiment'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cek_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "sns.kdeplot(df_sen['sentiment'],color='m',shade=True)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\") \n",
    "sns.boxplot(x=df_sen['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sepertinya sentimen terdistribusi secara merata antara positif dan negatif, tentu saja, mari kita lihat rata -rata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sen.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sepertinya hampir didistribusikan secara merata, tetapi yang positif memiliki kejadian yang sedikit lebih besar di sini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang mari kita lihat korelasi antara kata -kata yang termasuk dalam sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=10):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr\n",
    "\n",
    "#print(\"Top Absolute Correlations\")\n",
    "#print(get_top_abs_correlations(df_sen, 10))\n",
    "au = get_top_abs_correlations(df_sen, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Perfect Correlation')\n",
    "au[au==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ternyata ada 156 kata yang selalu terjadi bersama dalam setiap teks, meskipun kita tidak benar -benar melihat berapa banyak kalimat yang termasuk di sana tetapi korelasinya cukup tinggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = au[au<float(1)][0:10]\n",
    "label = top10.index\n",
    "label_list =[]\n",
    "for i in label:\n",
    "    for j in i:\n",
    "        if(j not in label_list):\n",
    "            label_list.append(j)\n",
    "            \n",
    "df_sen_corr = df_sen[label_list]\n",
    "corr = df_sen_corr.corr()\n",
    "for i in label_list:\n",
    "    for j in label_list:\n",
    "        if i!=j:\n",
    "            corr[i][j] = round(corr[i][j],3)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang mari kita lihat kata lain, itu sekarang selalu tetapi sering bersatu karena korelasi cukup tinggi meskipun tidak sama dengan 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "h = sns.heatmap(corr, annot=True,vmin=-1, vmax=1, center= 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top15 = au[au<float(1)][0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sepertinya kebanyakan dari mereka secara alami berkumpul bersama tetapi ada beberapa yang tidak benar -benar terlintas dalam pikiran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang mari kita lihat kata -kata yang paling terjadi di antara serangkaian kata yang termasuk dalam leksikon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top15_word = df_sen.drop(['sentiment'],axis=1).sum().sort_values(ascending=False)[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pal =sns.dark_palette(\"purple\", input=\"xkcd\",n_colors=15)\n",
    "pal =sns.light_palette(\"navy\", reverse=True,n_colors=15)\n",
    "g = sns.barplot(y = top15_word.index , x = top15_word,palette=pal)\n",
    "g.grid(False)\n",
    "plt.xlabel('Occurences')\n",
    "plt.ylabel('Words')\n",
    "plt.title(\"Top 15 Most Often Occured Words\",fontweight='bold') \n",
    "for i in range(15):\n",
    "    g.text(top15_word[i],i+0.22, top15_word[i],color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang kami ingin mengeksplorasi lebih dari kata itu sendiri, maka kami meneruskan sentimen ke dalam dataset asli dan kemudian mengeksplorasi beberapa data di sana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df_sen['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita lihat faktor lain yang berkorelasi dengan sentimen itu sendiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('correlation between numerical data',fontweight='bold')\n",
    "df_corr = df.corr()\n",
    "matrix = np.triu(df.corr())\n",
    "cmap =  sns.cubehelix_palette(light=0.5, as_cmap=True)\n",
    "h = sns.heatmap(df_corr, annot=True,vmin=-1, vmax=1, center= 0,mask=matrix,cmap = cmap)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari fitur numerik, tampaknya korelasinya sangat rendah, sekarang mari kita lihat yang lain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\", color_codes=True)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title('Sentiment in every language used',fontweight='bold')\n",
    "l = sns.boxplot(x='lang',y='sentiment',data=df,palette= sns.color_palette(\"RdPu\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sepertinya orang -orang dari kerangka waktu ini bahasa 'ko' dan 'und' selalu digunakan untuk memberikan sentimen positif sementara itu sebaliknya untuk 'pt' dan 'es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cek_df = df.dropna(subset=['possibly_sensitive'])\n",
    "cek_df = cek_df.reset_index(drop=True)\n",
    "plt.figure(figsize=(8,8))\n",
    "g = sns.boxplot(x='possibly_sensitive',y='sentiment',data=cek_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konten sensitif tidak menunjukkan pernyataan karena mereka hampir terdistribusi secara merata di antara mereka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place = df.groupby(['place']).mean().sort_values(by='sentiment',ascending=False)\n",
    "df_place = df_place.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place_dict = df.groupby(['place']).count().sort_values(by='id',ascending=False)['id'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place['number_of_tweets'] =  df_place.apply(lambda x:df_place_dict[x['place']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_place_pos = df_place.sort_values(by='sentiment',ascending=False)[0:10].reset_index(drop=True)\n",
    "top10_place_neg = df_place.sort_values(by='sentiment',ascending=True)[0:10].reset_index(drop=True)\n",
    "top10_place     = df_place.sort_values(by='number_of_tweets',ascending=False)[0:10].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang mari kita lihat beberapa tempat di mana sentimen yang dibuat darinya cenderung sensitif dan juga untuk tempat -tempat yang membuat sebaliknya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(10,10))\n",
    "fig.suptitle('Most Positive and Most Negative Sentiment Place',fontweight='bold')\n",
    "h = sns.barplot(y='place',x='sentiment',data=top10_place_pos,ax=ax1,palette=sns.color_palette(\"Blues_d\",n_colors=10))\n",
    "n = sns.barplot(y='place',x='sentiment',data=top10_place_neg,ax=ax2,palette=sns.color_palette('RdPu_d',n_colors=10))\n",
    "ax1.set_title('Top 10 Positive')\n",
    "ax2.set_title('Top 10 Negative')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last but not least, let's take a look at some places where tweets is most often come from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal =sns.dark_palette(\"green\", input=\"xkcd\",n_colors=10)\n",
    "g = sns.barplot(y = top10_place['place'] , x = top10_place['number_of_tweets'],palette=pal)\n",
    "g.grid=False\n",
    "plt.xlabel('number of tweets')\n",
    "plt.ylabel('place')\n",
    "plt.title(\"Top 10 Number of Tweets place\",fontweight='bold') \n",
    "\n",
    "for i in range(10):\n",
    "    g.text(top10_place['number_of_tweets'][i], i+0.22 ,round(top10_place['sentiment'][i],3),color='black')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c5f9eb9d72c0934fd2896ee225763aa5edf440926a0c875311ee0bd5be674b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
